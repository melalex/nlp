{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jigsaw Unintended Bias in Toxicity Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    DataCollatorWithPadding,\n",
    "    AutoModelForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    pipeline,\n",
    ")\n",
    "\n",
    "from src.data.kaggle import submit_competition, publish_model\n",
    "from src.model.metrics import compute_metrics\n",
    "from src.data.toxicity import (\n",
    "    load_toxicity_dataset,\n",
    "    TOXICITY_LABEL_TO_ID,\n",
    "    TOXICITY_ID_TO_LABEL,\n",
    ")\n",
    "from src.util.torch_device import resolve_torch_device\n",
    "from src.definitions import MODELS_FOLDER, EXTERNAL_DATA_FOLDER, PROCESSED_DATA_FOLDER, SUBMITIONS_FOLDER\n",
    "from src.metrics.bias import (\n",
    "    compute_bias_metrics_for_model,\n",
    "    calculate_overall_auc,\n",
    "    get_final_metric,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Prepare Env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_seed = 42\n",
    "\n",
    "random.seed(random_seed)\n",
    "torch.manual_seed(random_seed)\n",
    "np.random.seed(random_seed)\n",
    "\n",
    "device = resolve_torch_device()\n",
    "\n",
    "competition = \"jigsaw-unintended-bias-in-toxicity-classification\"\n",
    "submition_path = (\n",
    "    SUBMITIONS_FOLDER\n",
    "    / \"jigsaw-unintended-bias-in-toxicity-classification\"\n",
    "    / \"submission.csv\"\n",
    ")\n",
    "\n",
    "identity_columns = [\n",
    "    \"male\",\n",
    "    \"female\",\n",
    "    \"homosexual_gay_or_lesbian\",\n",
    "    \"christian\",\n",
    "    \"jewish\",\n",
    "    \"muslim\",\n",
    "    \"black\",\n",
    "    \"white\",\n",
    "    \"psychiatric_or_mental_illness\",\n",
    "]\n",
    "\n",
    "toxicity_column = \"label\"\n",
    "text_column = \"text\"\n",
    "\n",
    "model_checkpoint = \"distilbert/distilbert-base-uncased\"\n",
    "model_name = \"jigsaw-unintended-bias-in-toxicity-classification\"\n",
    "num_epochs = 3\n",
    "learning_rate = 2e-5\n",
    "\n",
    "epoch_time = int(time.time())\n",
    "\n",
    "os.environ[\"PYTORCH_MPS_HIGH_WATERMARK_RATIO\"] = \"0.0\"\n",
    "\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "\n",
    "ds = load_toxicity_dataset(\n",
    "    EXTERNAL_DATA_FOLDER,\n",
    "    PROCESSED_DATA_FOLDER,\n",
    "    tokenizer,\n",
    "    random_seed,\n",
    "    identity_columns,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Prepare model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_checkpoint,\n",
    "    num_labels=len(TOXICITY_LABEL_TO_ID),\n",
    "    id2label=TOXICITY_ID_TO_LABEL,\n",
    "    label2id=TOXICITY_LABEL_TO_ID,\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=MODELS_FOLDER / f\"{model_name}-checkpoint\",\n",
    "    learning_rate=learning_rate,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=num_epochs,\n",
    "    weight_decay=0.01,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    seed=random_seed,\n",
    "    auto_find_batch_size=True,\n",
    "    metric_for_best_model=\"f1\",\n",
    "    greater_is_better=True,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=ds[\"train\"],\n",
    "    eval_dataset=ds[\"test\"],\n",
    "    processing_class=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_feedback = trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_feedback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Save weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(MODELS_FOLDER / model_name)\n",
    "tokenizer.save_pretrained(MODELS_FOLDER / model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Predict test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(MODELS_FOLDER / model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODELS_FOLDER / model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv(\n",
    "    EXTERNAL_DATA_FOLDER\n",
    "    / \"jigsaw-unintended-bias-in-toxicity-classification\"\n",
    "    / \"test.csv\"\n",
    ")\n",
    "submission = pd.read_csv(\n",
    "    EXTERNAL_DATA_FOLDER\n",
    "    / \"jigsaw-unintended-bias-in-toxicity-classification\"\n",
    "    / \"sample_submission.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "predictor = pipeline(\n",
    "    \"text-classification\", model=model, tokenizer=tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_df = pd.DataFrame.from_records(\n",
    "    predictor(test[\"comment_text\"].values.tolist())\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission[\"prediction\"] = prediction_df[\"label\"].map(TOXICITY_LABEL_TO_ID)\n",
    "\n",
    "submition_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "submission = submission.set_index(\"id\")\n",
    "\n",
    "submission.to_csv(submition_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "message = (\n",
    "    f\"[ {model_checkpoint} ] {num_epochs} epochs with {learning_rate} learning rate\"\n",
    ")\n",
    "\n",
    "submit_competition(submition_path, message, competition)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. Evaluate model for bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "validate_df = ds[\"test\"].to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "      <th>severe_toxicity</th>\n",
       "      <th>obscene</th>\n",
       "      <th>identity_attack</th>\n",
       "      <th>insult</th>\n",
       "      <th>threat</th>\n",
       "      <th>asian</th>\n",
       "      <th>atheist</th>\n",
       "      <th>...</th>\n",
       "      <th>wow</th>\n",
       "      <th>sad</th>\n",
       "      <th>likes</th>\n",
       "      <th>disagree</th>\n",
       "      <th>sexual_explicit</th>\n",
       "      <th>identity_annotator_count</th>\n",
       "      <th>toxicity_annotator_count</th>\n",
       "      <th>__index_level_0__</th>\n",
       "      <th>input_ids</th>\n",
       "      <th>attention_mask</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>391182</td>\n",
       "      <td>0</td>\n",
       "      <td>Here's more \"tea leaves\":\\n1)  The opposition ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>121991</td>\n",
       "      <td>[101, 2182, 1005, 1055, 2062, 1000, 5572, 3727...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5666077</td>\n",
       "      <td>0</td>\n",
       "      <td>Huh?</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1269345</td>\n",
       "      <td>[101, 9616, 1029, 102]</td>\n",
       "      <td>[1, 1, 1, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5541104</td>\n",
       "      <td>0</td>\n",
       "      <td>Tempmanoa&gt;  Your post is well taken and provid...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1165966</td>\n",
       "      <td>[101, 8915, 8737, 2386, 10441, 1028, 2115, 269...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5593855</td>\n",
       "      <td>0</td>\n",
       "      <td>The last two weeks shows the internship is ove...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>1209559</td>\n",
       "      <td>[101, 1996, 2197, 2048, 3134, 3065, 1996, 2267...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>377550</td>\n",
       "      <td>0</td>\n",
       "      <td>And he was impeached for lying.</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>110675</td>\n",
       "      <td>[101, 1998, 2002, 2001, 17727, 5243, 7690, 200...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 48 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id  label                                               text  \\\n",
       "0   391182      0  Here's more \"tea leaves\":\\n1)  The opposition ...   \n",
       "1  5666077      0                                               Huh?   \n",
       "2  5541104      0  Tempmanoa>  Your post is well taken and provid...   \n",
       "3  5593855      0  The last two weeks shows the internship is ove...   \n",
       "4   377550      0                    And he was impeached for lying.   \n",
       "\n",
       "   severe_toxicity  obscene  identity_attack  insult  threat  asian  atheist  \\\n",
       "0              0.0      0.0              0.0     0.0     0.0    NaN      NaN   \n",
       "1              0.0      0.0              0.0     0.0     0.0    NaN      NaN   \n",
       "2              0.0      0.0              0.0     0.0     0.0    NaN      NaN   \n",
       "3              0.0      0.0              0.0     0.0     0.0    NaN      NaN   \n",
       "4              0.0      0.0              0.0     0.0     0.0    NaN      NaN   \n",
       "\n",
       "   ...  wow  sad  likes  disagree  sexual_explicit  identity_annotator_count  \\\n",
       "0  ...    0    0      5         0              0.0                         0   \n",
       "1  ...    0    0      0         0              0.0                         0   \n",
       "2  ...    0    0      0         0              0.0                         0   \n",
       "3  ...    0    1     10         1              0.0                         0   \n",
       "4  ...    0    0      2         0              0.0                         0   \n",
       "\n",
       "   toxicity_annotator_count  __index_level_0__  \\\n",
       "0                         4             121991   \n",
       "1                         4            1269345   \n",
       "2                         4            1165966   \n",
       "3                         6            1209559   \n",
       "4                         4             110675   \n",
       "\n",
       "                                           input_ids  \\\n",
       "0  [101, 2182, 1005, 1055, 2062, 1000, 5572, 3727...   \n",
       "1                             [101, 9616, 1029, 102]   \n",
       "2  [101, 8915, 8737, 2386, 10441, 1028, 2115, 269...   \n",
       "3  [101, 1996, 2197, 2048, 3134, 3065, 1996, 2267...   \n",
       "4  [101, 1998, 2002, 2001, 17727, 5243, 7690, 200...   \n",
       "\n",
       "                                      attention_mask  \n",
       "0  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "1                                       [1, 1, 1, 1]  \n",
       "2  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "3  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "4                  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]  \n",
       "\n",
       "[5 rows x 48 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validate_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = predictor(validate_df[text_column].values.tolist())\n",
    "y_pred = [it[\"score\"] if it[\"label\"] == \"toxic\" else 1 - it[\"score\"] for it in y_pred]\n",
    "\n",
    "validate_df[model_name] = y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subgroup</th>\n",
       "      <th>subgroup_size</th>\n",
       "      <th>subgroup_auc</th>\n",
       "      <th>bpsn_auc</th>\n",
       "      <th>bnsp_auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>homosexual_gay_or_lesbian</td>\n",
       "      <td>1099</td>\n",
       "      <td>0.873451</td>\n",
       "      <td>0.882674</td>\n",
       "      <td>0.975838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>black</td>\n",
       "      <td>1442</td>\n",
       "      <td>0.877475</td>\n",
       "      <td>0.879910</td>\n",
       "      <td>0.975720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>white</td>\n",
       "      <td>2451</td>\n",
       "      <td>0.897013</td>\n",
       "      <td>0.884919</td>\n",
       "      <td>0.980149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>muslim</td>\n",
       "      <td>2166</td>\n",
       "      <td>0.898768</td>\n",
       "      <td>0.909938</td>\n",
       "      <td>0.972307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>jewish</td>\n",
       "      <td>748</td>\n",
       "      <td>0.919292</td>\n",
       "      <td>0.937124</td>\n",
       "      <td>0.965055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>psychiatric_or_mental_illness</td>\n",
       "      <td>458</td>\n",
       "      <td>0.930447</td>\n",
       "      <td>0.929959</td>\n",
       "      <td>0.974025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>female</td>\n",
       "      <td>5451</td>\n",
       "      <td>0.941764</td>\n",
       "      <td>0.946684</td>\n",
       "      <td>0.972083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>male</td>\n",
       "      <td>4409</td>\n",
       "      <td>0.944213</td>\n",
       "      <td>0.942852</td>\n",
       "      <td>0.974899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>christian</td>\n",
       "      <td>4030</td>\n",
       "      <td>0.944305</td>\n",
       "      <td>0.961183</td>\n",
       "      <td>0.961080</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        subgroup  subgroup_size  subgroup_auc  bpsn_auc  \\\n",
       "2      homosexual_gay_or_lesbian           1099      0.873451  0.882674   \n",
       "6                          black           1442      0.877475  0.879910   \n",
       "7                          white           2451      0.897013  0.884919   \n",
       "5                         muslim           2166      0.898768  0.909938   \n",
       "4                         jewish            748      0.919292  0.937124   \n",
       "8  psychiatric_or_mental_illness            458      0.930447  0.929959   \n",
       "1                         female           5451      0.941764  0.946684   \n",
       "0                           male           4409      0.944213  0.942852   \n",
       "3                      christian           4030      0.944305  0.961183   \n",
       "\n",
       "   bnsp_auc  \n",
       "2  0.975838  \n",
       "6  0.975720  \n",
       "7  0.980149  \n",
       "5  0.972307  \n",
       "4  0.965055  \n",
       "8  0.974025  \n",
       "1  0.972083  \n",
       "0  0.974899  \n",
       "3  0.961080  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "bias_metrics_df = compute_bias_metrics_for_model(\n",
    "    validate_df, identity_columns, model_name, toxicity_column\n",
    ")\n",
    "\n",
    "bias_metrics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.9431894532255664)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_final_metric(\n",
    "    bias_metrics_df, calculate_overall_auc(validate_df, model_name, toxicity_column)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. Publish model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5000980e3d244f485ecb699a585a16d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://www.kaggle.com/static/images/site-logo.png\\nalt=\\'Kaggle…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading Model https://www.kaggle.com/models/amelashchenko/jigsaw-unintended-bias-in-toxicity-classification/transformers/default ...\n",
      "Starting upload for file /home/melal/Workspace/nlp/models/jigsaw-unintended-bias-in-toxicity-classification/config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading: 100%|██████████| 742/742 [00:00<00:00, 1.79kB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upload successful: /home/melal/Workspace/nlp/models/jigsaw-unintended-bias-in-toxicity-classification/config.json (742B)\n",
      "Starting upload for file /home/melal/Workspace/nlp/models/jigsaw-unintended-bias-in-toxicity-classification/model.safetensors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Uploading: 100%|██████████| 268M/268M [00:24<00:00, 11.0MB/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upload successful: /home/melal/Workspace/nlp/models/jigsaw-unintended-bias-in-toxicity-classification/model.safetensors (255MB)\n",
      "Starting upload for file /home/melal/Workspace/nlp/models/jigsaw-unintended-bias-in-toxicity-classification/special_tokens_map.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Uploading: 100%|██████████| 125/125 [00:00<00:00, 286B/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upload successful: /home/melal/Workspace/nlp/models/jigsaw-unintended-bias-in-toxicity-classification/special_tokens_map.json (125B)\n",
      "Starting upload for file /home/melal/Workspace/nlp/models/jigsaw-unintended-bias-in-toxicity-classification/tokenizer.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Uploading: 100%|██████████| 711k/711k [00:01<00:00, 673kB/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upload successful: /home/melal/Workspace/nlp/models/jigsaw-unintended-bias-in-toxicity-classification/tokenizer.json (695KB)\n",
      "Starting upload for file /home/melal/Workspace/nlp/models/jigsaw-unintended-bias-in-toxicity-classification/training_args.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Uploading: 100%|██████████| 5.37k/5.37k [00:00<00:00, 12.9kB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upload successful: /home/melal/Workspace/nlp/models/jigsaw-unintended-bias-in-toxicity-classification/training_args.bin (5KB)\n",
      "Starting upload for file /home/melal/Workspace/nlp/models/jigsaw-unintended-bias-in-toxicity-classification/tokenizer_config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Uploading: 100%|██████████| 1.23k/1.23k [00:00<00:00, 3.00kB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upload successful: /home/melal/Workspace/nlp/models/jigsaw-unintended-bias-in-toxicity-classification/tokenizer_config.json (1KB)\n",
      "Starting upload for file /home/melal/Workspace/nlp/models/jigsaw-unintended-bias-in-toxicity-classification/vocab.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Uploading: 100%|██████████| 232k/232k [00:00<00:00, 284kB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upload successful: /home/melal/Workspace/nlp/models/jigsaw-unintended-bias-in-toxicity-classification/vocab.txt (226KB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your model instance has been created.\n",
      "Files are being processed...\n",
      "See at: https://www.kaggle.com/models/amelashchenko/jigsaw-unintended-bias-in-toxicity-classification/transformers/default\n"
     ]
    }
   ],
   "source": [
    "publish_model(MODELS_FOLDER / model_name, \"transformers\", \"default\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
